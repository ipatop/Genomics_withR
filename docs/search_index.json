[
["index.html", "Genomics with R for biologists Overview 0.1 Course Aims 0.2 Intended Learning Outcomes 0.3 Acknowledgments", " Genomics with R for biologists Ines Lucia Patop: inespatop@brandeis.edu 2020-06-18 Overview The idea of the practical part will be to give you tools to be able to manage regular data-analysis in molecular biology labs. Also we think that this training in basic programing and statistics will be of high value for your future projects. 0.1 Course Aims Learn basic coding intuition and usage of R for analysis and plots. Undestand the main issues we have to deal with in Genomics. Learn how to do basic analysis, Chipseq, RNAseq, SingleCell. 0.2 Intended Learning Outcomes By the end of this course students will be able to: Create your own R code. Do basic and complex analysis. Read and understand R code. 0.3 Acknowledgments "],
["introduction-to-r-and-rstudio.html", "Lab 1 Introduction to R and RStudio 1.1 Objectives 1.2 Introduction 1.3 RStudio world 1.4 Create a project 1.5 Useful shortcuts in RStudio: 1.6 Installing and loading packages 1.7 Creating objects 1.8 R as a calculator 1.9 Functions 1.10 Loops 1.11 Conditions 1.12 Character objects 1.13 Identify 1.14 Modify 1.15 Useful resources 1.16 Activity:", " Lab 1 Introduction to R and RStudio 1.1 Objectives After this section you should be able to: Create projects and scripts in RStudio Install and load packages Do basic statistical analysis and plots Create basic for loops and if conditionals 1.2 Introduction We will be working with R and Rstudio, a user-friendly platform to use R. So, first you will need to download both R and RStudio. R is open-source and free programing language. This means that everybody, and I mean all of us, can collaborate, create packages and share things with others. This also means that everything we make in R is for public use and that, vice versa, we can use everything that anybody did in R. R is “object-orientated” a programing language. This basically means that most of the “things” in the environment are objects. I will not go deeper into it, but you can read more here. You will see in this class different types of objects. Each type of object has different properties which means we can not do everything with each type of object. In this class we will give you some data and simple tasks so you can start playing around and get use to basic commands and operators. But first, we will need to establish some common language: Let’s try to see some of the definitions more commonly used so we can understand them. I recommend you to do some research on your own. Shell or Terminal: the computer shell is a user interface to access to an operating system’s services. In linux and mac is easier to access. You can read more here. Programing language: is a language in a wide sense, ie, it is a set of rules in which you can give orders to the computer. A computer basically, computes. Yes, that is why they are called computers. Then, you can use your computer as a calculator, as a table manager, to write text, etc. The idea is that if you know the language of a programing language you can tell your computer to do stuff for you. We will use R language. Script: a text file that has the code (text in the programing language you are using) you will execute in the computer. In order for this to be executed you need to “copy-paste” the parts of code you wanna run into the shell. We will work in R-studio so this is done automatically with the ctr-enter command. Working Directory: in which folder (or directory) of your computer you will be running the code. As default R will run in your base-directory. This is the core of your computer. I recommend you to have a specific folder inside the documents folder with each of you projects and to run everything there. We will learn also how to set this up. Environment: All the objects, functions and everything you have loaded at that moment. This is a short-term memory thing. If we do not save this, everything we did will be deleted after we close the program. It would be like the words you have a in a text file if you do not save the file they will be lost. Functions: as mathematical functions, this takes inputs and generates outputs. We will see some in this class. For loop, if, else: this are basic logic operators that allows us to generate specific outputs. We can use it inside functions or as independent. Package: R has the option to load many premade functions. The R package is a set of documented functions that someone did and put to be available for the rest of the community. We will use in this course many of them like: DeSeq2, ggplot2, etc. 1.3 RStudio world R Studio has everything integrated. You can have the script, the environment, the actual shell and a useful window to access data and to view the plots and see the manual and description of every function and package. Here we can create a project in which everything will be integrated. This might not be the most efficient thing in terms of memory use but is the simplest for now. When you start to understand the logic of the programing language I encourage you to run R in different supports like typing “R” in your computer shell and run directly from command line. Regarding RStudio see the image bellow: Figure 1.1: Regular RStudio setup. Upper left panel: the script. Everything you write here you can just execute by pressing CTR + ENTER. Upper right panel: the environment. Lower left panel: the “console”, work exactly as the terminal. Is the representation of the terminal just for R. If you want, you can run other things that are not R in the shell (not the console), if you are interested read this nice article https://support.rstudio.com/hc/en-us/articles/115010737148-Using-the-RStudio-Terminal Lower Right: The multi layer panel that allows you to see and browse the files, plots and help pages. 1.4 Create a project 1. Open RStudio: Create a project: File &gt; New Project… &gt; Existing Directory &gt; choose the folder. Automatically this should change the working directory to this folder. However, it is nice to check this we should run the following command: getwd() #Get working directory 2. Create the script: File &gt; New File… &gt;R Script Now we can start putting things in the script and running them in the “console” or representation of the shell in RStudio. So, let’s check the working directory and change it if needed. To run the command, just write “getwd()” and press CTR+ENTER. This should generate this in the “Console”: getwd() [1] &quot;/Users/skl/Documents/Class/0_class&quot; #here your working directory Now we know how to run things in RStudio. Save the script and the working directory: when you close the project it will automatically ask. I recommend to save at least the script everytime you can. File &gt; save 1.5 Useful shortcuts in RStudio: tab: auto-complete control + the up arrow or command + up arrow: auto-complete tool that works only in the interactive console control + enter or command + enter: executes the selected lines of code control + s : save 1.6 Installing and loading packages When we open R the basic package with all the basic functions is loaded but we will use other functions from other packages. For now, we will start with ggplot2. This is not installed in our computer yet so we have to install it. This is done only ONCE in the computer. Then we need to load it in the current environment. This is done EVERY TIME we reopen R. install.packages(&quot;ggplot2&quot;) #only once library(&quot;ggplot2&quot;) #everytime we reopen R session You might have noticed that I use “#” this a good way to add comments to the code. Any line that begins with a “#” will NOT be executed. We will be using packages that are part of Bioconductor. This is a big repository for biological relevant packages. They are installed in this way: if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) #only once BiocManager::install(&quot;DESeq2&quot;) #only once library(&quot;DESeq2&quot;) 1.7 Creating objects Objects in R are created using the assign symbols: &lt;- and = as follows: object.name &lt;- Object.Assingnemet. This means that objects names should start with a letter and should NOT contain spaces. You can replace them with a dot or an underscore. Objects can be of different class and will be overwritten without any warnings. If we execute the object name we will access it as better as the Console can do it. For numbers it is simple but you will notice that for other objects it is not. x&lt;-0.5 x class(x) #this tells us the class of the x object ## [1] 0.5 ## [1] &quot;numeric&quot; If we assign other thing to x, it will be overwritten: x&lt;-1 #this is overwriting x x class(x) #this tells us the class of the x object ## [1] 1 ## [1] &quot;numeric&quot; We can create as many objects as we want y&lt;-2.3 y class(y) ## [1] 2.3 ## [1] &quot;numeric&quot; 1.8 R as a calculator R will operate as a calculator for numbers. It has a lot of prebuilt functions. Let’s see one obvious. x+y (x+y)/2 ## [1] 3.3 ## [1] 1.65 We can now apply this sum a new object z&lt;-x+y z ## [1] 3.3 And divide it by 2. z/2 ## [1] 1.65 Which is the same as doing the mean of x and y. mean(c(x,y))#what is this c()???? ## [1] 1.65 To create a vector of numbers (or anything), you can just use c(n1,n2), you can also store this vector in a new object. v&lt;-c(x,y) v class(v) ## [1] 1.0 2.3 ## [1] &quot;numeric&quot; You can now use it inside fucntions. mean(v) ## [1] 1.65 We can add (append) more elemts to the vector. t&lt;-c(v,5) t mean(t) #...and so on ## [1] 1.0 2.3 5.0 ## [1] 2.766667 1.9 Functions We can think as functions exactly as we know mathematical functions. They take an input and generate an output. \\[ y=f(x) \\] \\[ output=function(input) \\] In R language that looks like this: nameofunction&lt;-function(x){ what the function does } and the can do literally anything R can do. Math, plot, modify tables, etc. Lets create a new mean function. We will call is mean.us. This will take the mean of two numbers. They will be called n1 and n2. mean.us&lt;-function(n1,n2){ y&lt;-((n1+n2)/2) return(y) #return is the one part of this function that actually makes it to return a value } #lets see if this works mean.us(2,3) ## [1] 2.5 We can make it even more fancy and print a message mean.us&lt;-function(n1,n2){ y&lt;-((n1+n2)/2) return(paste0(&quot;The mean of &quot;,n1,&quot; and &quot;,n2,&quot; is: &quot;,y)) } mean.us(2,3) ## [1] &quot;The mean of 2 and 3 is: 2.5&quot; We can do now the mean plus 1 mean.plus1&lt;-function(n1,n2){ y&lt;-((n1+n2)/2+1) return(paste0(&quot;The mean of &quot;,n1,&quot; and &quot;,n2,&quot; plus one is: &quot;,y)) } mean.plus1(2,3) ## [1] &quot;The mean of 2 and 3 plus one is: 3.5&quot; 1.10 Loops Loops are useful to apply a function or an action to multiple objects. We will see for loops but be aware that another common loop type is the while loop. For loops will go over each element of a vector or list provided. In R language they look like this: for (x in vector) { DO SOMETHING }. And it literally means that it will go over each element of the vector, each time x will take the value of the element it goes that time and will do something. #i this is a little complex so lets go one step at a time: i is an object that will be getting the value of each elemnt we go thru for(x in c(1,2,3,4)){ print(paste0(&quot;x is: &quot;,x)) } ## [1] &quot;x is: 1&quot; ## [1] &quot;x is: 2&quot; ## [1] &quot;x is: 3&quot; ## [1] &quot;x is: 4&quot; We can now do something more complex inside the loop. for(x in c(1,2,3,4)){ c=x/2 print(paste0(&quot;c is: &quot;,c)) } ## [1] &quot;c is: 0.5&quot; ## [1] &quot;c is: 1&quot; ## [1] &quot;c is: 1.5&quot; ## [1] &quot;c is: 2&quot; We can also loop over vectors that are already in our list of objects. #t is mande of many elements already t #if we want to sum 1 to each element in t and print it out we can do as follows for(i in t){ print(i) c=i+1 print(c) } ## [1] 1.0 2.3 5.0 ## [1] 1 ## [1] 2 ## [1] 2.3 ## [1] 3.3 ## [1] 5 ## [1] 6 This is usefull for other type of lists and vectors vec&lt;-c(&quot;a&quot;,&quot;b&quot;) for (i in c(vec)){ print(paste0(&quot;i is: &quot;,i)) } ## [1] &quot;i is: a&quot; ## [1] &quot;i is: b&quot; And we can access it in differen ways: for (i in 1:length(vec)){ print(paste0(&quot;i is: &quot;,i)) print(paste0(&quot;elemnt i is: &quot;,vec[i])) } ## [1] &quot;i is: 1&quot; ## [1] &quot;elemnt i is: a&quot; ## [1] &quot;i is: 2&quot; ## [1] &quot;elemnt i is: b&quot; 1.11 Conditions R assigns using = and compares using ==,&lt; and &gt;. It can then use if and else to generate conditions and actions. This can be more complex by adding AND, OR gates with &amp; and | respectively. x=1 #lets explore x x ## [1] 1 Compare, equal to: x==1 ## [1] TRUE Smaller than: x&lt;2 ## [1] TRUE Bigger than: x&gt;2 ## [1] FALSE Apply this comparisons to an if/else: if (x &lt; 2){ c=x+1 print(paste0(&quot;c is: &quot;, c)) } else { print(&quot;x is too big&quot;) } ## [1] &quot;c is: 2&quot; Change the condition: if (x &lt; 1){ c=x+1 print(paste0(&quot;c is: &quot;, c)) } else { print(&quot;x is too big&quot;) } ## [1] &quot;x is too big&quot; 1.12 Character objects So far, we saw opperations with numbers. But R can have objects with words or characters. We can have a complex phrase phrase&lt;-&quot;I am a beatufill phrase. Hello world&quot; phrase ## [1] &quot;I am a beatufill phrase. Hello world&quot; Or a vector of character elements: chrvec&lt;-c(phrase,&quot;abcde&quot;,&quot;67&quot;) chrvec ## [1] &quot;I am a beatufill phrase. Hello world&quot; ## [2] &quot;abcde&quot; ## [3] &quot;67&quot; class(chrvec) ## [1] &quot;character&quot; 1.13 Identify We can operate over them. For example, we can select the elemnt that has certain characteristic. For this we will use grep. This is based on regualar expression. Lets select the elemnts that have the letter d. grep(pattern = &quot;d&quot;,x = chrvec,value = T) ## [1] &quot;I am a beatufill phrase. Hello world&quot; ## [2] &quot;abcde&quot; grep(pattern = &quot;d&quot;,x = chrvec,value = F) #What changed between the previous one and this one? ## [1] 1 2 Clearly, the fist two elements have the letter d. What if we want to select the one that has it at the end? grep(pattern = &quot;d$&quot;,x = chrvec,value = T) ## [1] &quot;I am a beatufill phrase. Hello world&quot; The same can be done for the begining. This of course gives us an empty result. grep(pattern = &quot;^d&quot;,x = chrvec,value = T) ## character(0) 1.14 Modify We can modify the objects. We will use gsub gsub(pattern = &quot;d&quot;,replacement = &quot;I am replacing d&quot;,x = chrvec) ## [1] &quot;I am a beatufill phrase. Hello worlI am replacing d&quot; ## [2] &quot;abcI am replacing de&quot; ## [3] &quot;67&quot; 1.15 Useful resources A package to learn R: Swirl Support from RStudio: Usefull resource with usefull packages in R Girhub: Bioinformatic resources Free books: R for data science unix command line 1 unix command line 2 data structure with python 1.16 Activity: Create a new funciton that has inside a for loop and other that has an if/else condition. plus.one.onlyifpos &lt;- function(n){ if(n &gt; 0){ return(n+1) } else { return(&quot;number is negative&quot;) } } plus.one.onlyifpos(20) plus.one.onlyifpos(-20) "],
["data-manipulation.html", "Lab 2 Data manipulation 2.1 Objectives 2.2 Introduction 2.3 Load data 2.4 Data exploration 2.5 Subsetting 2.6 Activity:", " Lab 2 Data manipulation 2.1 Objectives After this section you should be able to: Load, explore and manipulate data in R 2.2 Introduction One of the main uses of R is for data manipulation and plot. This is similar to what many of us do in any regular table editor as excel or google spread sheet. We will use the following packages. You can read in detail the manual of each of them. #Install packages #install.packages(&quot;ggplot2&quot;) #install.packages(&quot;dplyr&quot;) #install.packages(&quot;plyr&quot;) #Load the package library(&quot;ggplot2&quot;) library(&quot;dplyr&quot;) library(&quot;plyr&quot;) library(RColorBrewer) library(car) #Manuals #vignette(&quot;dplyr&quot;) #?ggplot2 #?plyr 2.3 Load data There are many ways to load data. In the following chapters we will use a diverse set of functions to read the data from files. Some of them are: read.table() #general to any type of table read.csv() #specific for comma sepparated tables read.delim() #specific for tab delimited tables Some of the important options of these function are: read.table(file = &quot;location/of/your/file.txt&quot;,sep = &quot;.&quot;,header = T or F) Where the separator can be a comma, dot, etc. You can see more details using: ?read.table In this case we will use data that is already available in R. The package datasets provides a handful set of data to analyze. We will use the ChickWeight dataset. This is data set of weight in chickens with age an different diet. This will allow us to visualize the data and to do some statistic tests. # Install the package #install.packages(&quot;datasets&quot;) # For a full list of these datasets, type library(help = &quot;datasets&quot;) # Load the library and dataset library(datasets) data(ChickWeight) #What happens in the Environment section of RStudio? 2.4 Data exploration It is important to understand the data before heading into the analysis. We will go over some techniques for this. # To see the table, you can click on the environment part or run this... #View(ChickWeight) # As you can see this is a table, just in case we want to convert it to a data.frame ChickWeight&lt;-as.data.frame(ChickWeight) To see only the beginning, we can use the head function: head(ChickWeight) ## weight Time Chick Diet ## 1 42 0 1 1 ## 2 51 2 1 1 ## 3 59 4 1 1 ## 4 64 6 1 1 ## 5 76 8 1 1 ## 6 93 10 1 1 What is n doing? head(ChickWeight,n = 20) ## weight Time Chick Diet ## 1 42 0 1 1 ## 2 51 2 1 1 ## 3 59 4 1 1 ## 4 64 6 1 1 ## 5 76 8 1 1 ## 6 93 10 1 1 ## 7 106 12 1 1 ## 8 125 14 1 1 ## 9 149 16 1 1 ## 10 171 18 1 1 ## 11 199 20 1 1 ## 12 205 21 1 1 ## 13 40 0 2 1 ## 14 49 2 2 1 ## 15 58 4 2 1 ## 16 72 6 2 1 ## 17 84 8 2 1 ## 18 103 10 2 1 ## 19 122 12 2 1 ## 20 138 14 2 1 What is the structure of the data.frame? str(ChickWeight) ## &#39;data.frame&#39;: 578 obs. of 4 variables: ## $ weight: num 42 51 59 64 76 93 106 125 149 171 ... ## $ Time : num 0 2 4 6 8 10 12 14 16 18 ... ## $ Chick : Ord.factor w/ 50 levels &quot;18&quot;&lt;&quot;16&quot;&lt;&quot;15&quot;&lt;..: 15 15 15 15 15 15 15 15 15 15 ... ## $ Diet : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 1 1 1 1 1 1 1 ... With the $ operator we can explore the columns class(ChickWeight$weight) ## [1] &quot;numeric&quot; We can see the dimensions of the table for example: how many rows it has? nrow(ChickWeight) ## [1] 578 How many columns? ncol(ChickWeight) ## [1] 4 The names of columns names(ChickWeight) ## [1] &quot;weight&quot; &quot;Time&quot; &quot;Chick&quot; &quot;Diet&quot; With the [] we can access the individual elements names(ChickWeight)[3] ## [1] &quot;Chick&quot; We can see the levels of a factor levels(ChickWeight$Diet)[1:3] ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; What is the difference if we just print the column? ChickWeight$Diet[1:3] ## [1] 1 1 1 ## Levels: 1 2 3 4 Can we see the levels of a numeric vector? This is a reminder that the data type is important. levels(ChickWeight$weight) # nop We can now get different basic statistics now: mean(ChickWeight$weight) ## [1] 121.8183 summary(ChickWeight$weight) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 35.0 63.0 103.0 121.8 163.8 373.0 summary(ChickWeight) ## weight Time Chick Diet ## Min. : 35.0 Min. : 0.00 13 : 12 1:220 ## 1st Qu.: 63.0 1st Qu.: 4.00 9 : 12 2:120 ## Median :103.0 Median :10.00 20 : 12 3:120 ## Mean :121.8 Mean :10.72 10 : 12 4:118 ## 3rd Qu.:163.8 3rd Qu.:16.00 17 : 12 ## Max. :373.0 Max. :21.00 19 : 12 ## (Other):506 To see what is this exactly doing, just go to the help page: ?summary To save this summary table we can create an object with just the result of the summary chick_sumary&lt;-summary(ChickWeight) chick_sumary ## weight Time Chick Diet ## Min. : 35.0 Min. : 0.00 13 : 12 1:220 ## 1st Qu.: 63.0 1st Qu.: 4.00 9 : 12 2:120 ## Median :103.0 Median :10.00 20 : 12 3:120 ## Mean :121.8 Mean :10.72 10 : 12 4:118 ## 3rd Qu.:163.8 3rd Qu.:16.00 17 : 12 ## Max. :373.0 Max. :21.00 19 : 12 ## (Other):506 class(chick_sumary) ## [1] &quot;table&quot; We can change the data kind, and assign it to a different object chick_sumary_df&lt;-as.data.frame(chick_sumary) This is not that useful as you can see if you inspect the data in using View(chick_sumary_df) this is because it is a complicated format, we better just save the table. We will see other ways to save data in R in the future chapters. You can see more details using: ?write.table write.table(chick_sumary, &quot;mydata.txt&quot;, sep=&quot;\\t&quot;,row.names = F,col.names = T) #this is clearly no perfect but for the important part, the numeric and integer columns, we have the stat 2.5 Subsetting Subsetting means extracting part of the data. There are many different ways to do this. One important notion for tables and data frames is that dimensions go as follows: data[row,column] #we can see specific columns and rows ChickWeight[1,1:3] #row 1, column 1:3 ## weight Time Chick ## 1 42 0 1 ChickWeight[1:3,1] #col 1, row 1:3 ## [1] 42 51 59 ChickWeight[1,1] #row1, col1 ## [1] 42 If we want to know for example only the data from the chickens taking the diet 4 head(ChickWeight[ChickWeight$Diet==4,]) ## weight Time Chick Diet ## 461 42 0 41 4 ## 462 51 2 41 4 ## 463 66 4 41 4 ## 464 85 6 41 4 ## 465 103 8 41 4 ## 466 124 10 41 4 Why == and no =? Remember in R, = is an assignment, as the &lt;-, while the == is for comparison. head(ChickWeight$Diet==4) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE Lets explore the class: class(ChickWeight$Diet==4) ## [1] &quot;logical&quot; So, when we do ChickWeight[ChickWeight$Diet==4,], R is just showing the ChickWeight for which ChickWeight$Diet==4 is TRUE head(which(ChickWeight$Diet==4)) ## [1] 461 462 463 464 465 466 head(ChickWeight[ChickWeight$Diet==4,]) ## weight Time Chick Diet ## 461 42 0 41 4 ## 462 51 2 41 4 ## 463 66 4 41 4 ## 464 85 6 41 4 ## 465 103 8 41 4 ## 466 124 10 41 4 And for more conditions, we can use AND (&amp;) to integrate them. head(ChickWeight[ChickWeight$Diet==4 &amp; ChickWeight$Time&gt;6,]) ## weight Time Chick Diet ## 465 103 8 41 4 ## 466 124 10 41 4 ## 467 155 12 41 4 ## 468 153 14 41 4 ## 469 175 16 41 4 ## 470 184 18 41 4 Other option is OR (|). Remember, computers will read as things come \\[ condition-A AND condition-B OR condition-C condition-A &amp; condition-B | condition-C \\] Is not the same as \\[ condition A &amp; (condition B | condition C) \\] head(ChickWeight[ChickWeight$Diet==4 &amp; ChickWeight$Time&gt;6 &amp; ChickWeight$Time&lt;20,]) ## weight Time Chick Diet ## 465 103 8 41 4 ## 466 124 10 41 4 ## 467 155 12 41 4 ## 468 153 14 41 4 ## 469 175 16 41 4 ## 470 184 18 41 4 And if we just want the weights of these… ChickWeight$weight[ChickWeight$Diet==4 &amp; ChickWeight$Time&gt;6 &amp; ChickWeight$Time&lt;20,] why this gives an error? Because we only have one dimension now, not 2. ChickWeight$weight is one dimention object, so we have to use [ ], not [ , ]. head(ChickWeight$weight[ChickWeight$Diet==4 &amp; ChickWeight$Time&gt;6 &amp; ChickWeight$Time&lt;20]) ## [1] 103 124 155 153 175 184 2.6 Activity: This activity integrates knowledge from the previous chapter. 1. Remove the first and last row of the ChickWeight data frame 2. Create a vector with the second column from the data frame "],
["visualization-and-statistical-test.html", "Lab 3 Visualization and Statistical test 3.1 Objectives 3.2 Introduction 3.3 Plots 3.4 Statistical test 3.5 Activity: 3.6 Resources", " Lab 3 Visualization and Statistical test 3.1 Objectives After this section you should be able to: Plot and explore the data in many ways. Do statistical tests on the data. 3.2 Introduction We will use the same data we already explored in last chapter. Remember: chickens weight, age (Time) for different diets. There are many things we can explore in this data. Each question we might want to answer will be better addressed using different plots. For example: 1. If the chickens are older we expect them to be bigger. This can be visualized using a dotplot. 2. We might want to see the distribution of weight separated by diet. This can be addressed by a histogram. 3.3 Plots We will use the package ggplot2. It is a very useful and documented package. We will focus on the ggplot function. This function generates plots as layers. This allows us to manipulate the colors, the plot type, etc. I know it can be difficult to understand it at the beginning but after a while it becomes really intuitive. Important things to consider: 1. We will be able to plot anything that is a column in the data frame. 2. Everything is or can be a layer in the plot. 3. When you decide to color or shape by a factor that separates your data this will impact the plot. Again,we can plot any column. So lets axplore the columns. It is important to know the class of each column. It is not the same trying to plot a number, than a letter. names(ChickWeight) #names of the columns in the data frame ## [1] &quot;weight&quot; &quot;Time&quot; &quot;Chick&quot; &quot;Diet&quot; head(ChickWeight) #head of the data frame ## weight Time Chick Diet ## 1 42 0 1 1 ## 2 51 2 1 1 ## 3 59 4 1 1 ## 4 64 6 1 1 ## 5 76 8 1 1 ## 6 93 10 1 1 str(ChickWeight) #structure of the data frame ## &#39;data.frame&#39;: 578 obs. of 4 variables: ## $ weight: num 42 51 59 64 76 93 106 125 149 171 ... ## $ Time : num 0 2 4 6 8 10 12 14 16 18 ... ## $ Chick : Ord.factor w/ 50 levels &quot;18&quot;&lt;&quot;16&quot;&lt;&quot;15&quot;&lt;..: 15 15 15 15 15 15 15 15 15 15 ... ## $ Diet : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 1 1 1 1 1 1 1 ... 3.3.1 Line and points To see things as correlations, we usually use points and lines. We will see how to do it using different plot options. Dot plot with basic qplot (from ggplot but les complex) qplot(data=ChickWeight,x = weight, y=Time, geom = c(&quot;line&quot;,&quot;point&quot;)) Figure 3.1: Point and line plots qplot(data=ChickWeight,x = weight, y=Time, geom = c(&quot;line&quot;,&quot;point&quot;), colour=Diet) #adding the color helps to separate the data Figure 3.2: Point and line plots The same using ggplot: ggplot(data = ChickWeight, aes(y = weight, x=Time,colour=Diet))+ #data and basic things about the plot geom_point() + #add the type of plot scale_colour_brewer(palette = &quot;Set1&quot;) #add a colot pallet Figure 3.3: Point and line plots ggplot(data = ChickWeight, aes(y = weight, x=Time,colour=Diet))+ #data and basic things about the plot geom_point() + #add the type of plot geom_smooth() + #add a trend line of mean plus se scale_colour_brewer(palette = &quot;Set1&quot;) Figure 3.4: Point and line plots 3.3.2 Historgam and density plots Histograms are used to represent the distribution of a variable over the population. If you want to read more you can go to this link. Other way to represent the same thing is to use cumulative plots we are not going to explore them now but if you are interested in doing them with ggplot you can go to this link Density plots are similar to histograms but implies a more complex treatment of the data. They look like smooth histogram. They are the probability density function of the variable. qplot(data = ChickWeight,x=weight, binwith=10) Figure 3.5: Histogram and Density plots qplot(data = ChickWeight,x=weight, binwith=10, colour=Diet) #the color separates the data Figure 3.6: Histogram and Density plots qplot(data = ChickWeight,x=weight, geom = &quot;density&quot;, colour=Diet) Figure 3.7: Histogram and Density plots With ggplot ggplot(data = ChickWeight, aes(x=weight,color=Diet))+ geom_histogram(fill=&quot;white&quot;, position=&quot;identity&quot;)+ scale_colour_brewer(palette = &quot;Set1&quot;)#this is selecting the color scheme, try taking it out, or mofyfying it Figure 3.8: Point and line plots ggplot(data = ChickWeight, aes(x=weight,fill=Diet))+ geom_density( alpha=0.5)+ #the density plot with the option to modify the transparency of the polot solor, it goes between 0 and 1. Try modifying it. scale_colour_brewer(palette = &quot;Set1&quot;) Figure 3.9: Point and line plots 3.3.3 Boxplot Boxplots are a nice way to visualize the data distribution and to get and intuition of how this is different between conditions. As you can see in this figure, it summarizes a LOT of information: Figure 3.10: Boxplot description. Figrue affapted from https://www.simplypsychology.org/boxplots.html ggplot(data = ChickWeight, aes(y=weight,x=as.factor(Time),fill=Diet))+ #Note how the x, y and color changes geom_boxplot()+ #this is adding the boxplot scale_colour_brewer(palette = &quot;Set1&quot;) Figure 3.11: Boxplot What happens if we do not use the as.factor? Again, a reminder that the data type is important! ggplot(data = ChickWeight, aes(y=weight,x=Time,fill=Diet))+ geom_boxplot()+ #this is adding the boxplot scale_colour_brewer(palette = &quot;Set1&quot;) Figure 3.12: Boxplot It seems interesting to separate this by age (Time). This is achieved by another layer named facet. ggplot(data = ChickWeight, aes(x=weight,fill=Diet))+ geom_density( alpha=0.5)+ scale_colour_brewer(palette = &quot;Set1&quot;)+ facet_wrap(~Time,scales = &quot;free&quot;) #This will separate the data into panels given the time, try looking for the meaning of the scale option Figure 3.13: Plot separating by age of the chicken ggplot(data = ChickWeight, aes(y=weight,x=as.factor(Time),fill=Diet))+ geom_boxplot()+scale_colour_brewer(palette = &quot;Set1&quot;)+ facet_wrap(~Time,scales = &quot;free&quot;) Figure 3.14: Plot separating by age of the chicken ggplot(data = ChickWeight, aes(y=weight,x=as.factor(Time),fill=Diet))+ geom_violin()+ scale_colour_brewer(palette = &quot;Set1&quot;)+ facet_wrap(~Time,scales = &quot;free&quot;) Figure 3.15: Plot separating by age of the chicken 3.3.4 Saving plots Imagine you want now to save some of these plots. You can use the button export in RStudio. But you can also use the pdf function. This function allows us to determine the width and height of the plots. Check what happens if you modify the option in the plots below. These pdf files will be saved on your working directory with the name, width and height determined in the function. Important things: Do not forget to put the “.pdf” at the end of the file name. What do you think it will happen if you forget it? When you finish running the plots that you want to be in the pdf file, you have to run dev.off(). This will close the plot. If you forget this, you will not be able to open the plot. pdf(&quot;densityplot.pdf&quot;,width = 20, height = 20) #save the plot as a pdf, control width and height of the pdf ggplot(data = ChickWeight, aes(x=weight,fill=Diet))+ geom_density( alpha=0.5)+ scale_colour_brewer(palette = &quot;Set1&quot;)+ facet_wrap(~Time,scales = &quot;free&quot;) dev.off() #end the plot pdf(&quot;density_and_violin.plot.pdf&quot;,width = 20, height = 20) #save the plot as a pdf, control width and height of the pdf ggplot(data = ChickWeight, aes(x=weight,fill=Diet))+ geom_density( alpha=0.5)+ scale_colour_brewer(palette = &quot;Set1&quot;)+ facet_wrap(~Time,scales = &quot;free&quot;) ggplot(data = ChickWeight, aes(y=weight,x=as.factor(Time),fill=Diet))+ geom_violin()+ scale_colour_brewer(palette = &quot;Set1&quot;)+ facet_wrap(~Time,scales = &quot;free&quot;) dev.off() #end the plot ## quartz_off_screen ## 2 ## quartz_off_screen ## 2 3.4 Statistical test 3.4.1 Descriptive statistics We already saw a way to get the descriptive stats from a table by using summary. We will try to compare the weight of chickens under different diets without considering the age. First, we will now do a mean and SD table for each diet. There is one function that can do this for us. ddply is a function that first divides the data by a variable written as .(Var) and then perform an specific function. With the indication of “transform” this will create a new column in out data stat_ChickWeight&lt;-ddply(ChickWeight, .(Diet), transform, Mean=mean(weight,na.rm = T), SD=sd(weight,na.rm = T)) head(stat_ChickWeight) ## weight Time Chick Diet Mean SD ## 1 42 0 1 1 102.6455 56.65655 ## 2 51 2 1 1 102.6455 56.65655 ## 3 59 4 1 1 102.6455 56.65655 ## 4 64 6 1 1 102.6455 56.65655 ## 5 76 8 1 1 102.6455 56.65655 ## 6 93 10 1 1 102.6455 56.65655 Is this what we wanted? What happens if instead of “transform” we use “summarize”? Check ?ddply for more detail. statWeight_ChickWeight&lt;-ddply(ChickWeight, .(Diet), summarise, Mean=mean(weight,na.rm = T), SD=sd(weight,na.rm = T)) head(statWeight_ChickWeight) ## Diet Mean SD ## 1 1 102.6455 56.65655 ## 2 2 122.6167 71.60749 ## 3 3 142.9500 86.54176 ## 4 4 135.2627 68.82871 This is usefull for ploting, here a good plot: ggplot(statWeight_ChickWeight, aes(x=Diet, y=Mean, fill=Diet)) + geom_bar(stat=&quot;identity&quot;, position=position_dodge()) + geom_errorbar(aes(ymin=Mean-SD, ymax=Mean+SD), width=.2, position=position_dodge(.9)) Figure 3.16: Boxplot 3.4.2 T-test/Wilcoxon To compare means we can do a T test but to do this we need to test the assumptions of this test: Normality of the data and Homoscedasticity (ie, the variance is similar between the two groups we want to compare) Question : Is there any significant difference in the weights between diet 1 and 3? Preliminary test to check independent t-test assumptions Assumption 1: Are the two samples independents? Yes, they are two different samples Assumption 2: Are the data from each of the 2 groups follow a normal distribution? Shapiro-Wilk normality test for the different diets shapiro.test(ChickWeight$weight[ChickWeight$Diet==1]) ## ## Shapiro-Wilk normality test ## ## data: ChickWeight$weight[ChickWeight$Diet == 1] ## W = 0.89336, p-value = 2.211e-11 The function with allows us to do a simpler writing with(ChickWeight, shapiro.test(weight[Diet == 1])) ## ## Shapiro-Wilk normality test ## ## data: weight[Diet == 1] ## W = 0.89336, p-value = 2.211e-11 with(ChickWeight, shapiro.test(weight[Diet == 2])) ## ## Shapiro-Wilk normality test ## ## data: weight[Diet == 2] ## W = 0.90399, p-value = 3.159e-07 pvalue &lt; 0.05, these are not normally distributed. We can NOT use t-test here. If we remember the histograms, this makes sense. qplot(data=ChickWeight, x = weight, facets = &quot;Diet&quot;,geom = &quot;density&quot;) Figure 3.17: CAPTION THIS FIGURE!! Assumption 3: Do the two populations have the same variances? We’ll use F-test to test for homogeneity in variances. This is implemented by a function named var.test. This will require you to have which variable you want to test and separated by which variable. This is clearly also not homoscedastic. var.test(weight~ Diet, data = ChickWeight[ChickWeight$Diet %in% c(1,2),]) ## ## F test to compare two variances ## ## data: weight by Diet ## F = 0.62601, num df = 219, denom df = 119, p-value = 0.002928 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.4525703 0.8530014 ## sample estimates: ## ratio of variances ## 0.626013 What happens if you try to run this var.test(weight~ Diet, data = ChickWeight)? We will use then Wilcoxon. wilcox.test(weight~ Diet, data = ChickWeight[ChickWeight$Diet %in% c(1,2),],exact = FALSE) ## ## Wilcoxon rank sum test with continuity correction ## ## data: weight by Diet ## W = 11213, p-value = 0.02181 ## alternative hypothesis: true location shift is not equal to 0 Are them different? 3.4.3 Anova/Kruskal–Wallis Another way to test differences is to do an ANOVA or its non-parametric alternative Kruskal–Wallis. We already know that this data cannot be analyzed using parametric test as anova. But let’s explore just for fun. Let’s check all the diets together # Compute the analysis of variance res.aov &lt;- aov(weight~ Diet, data = ChickWeight) # Summary of the analysis summary(res.aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Diet 3 155863 51954 10.81 6.43e-07 *** ## Residuals 574 2758693 4806 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We see that the diet is a significant component of the variance of the data. Now we should know from where it is coming. We need to do a multiple pairwise-comparison. We will use Tukey Honest Significant Difference for this. TukeyHSD(res.aov) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = weight ~ Diet, data = ChickWeight) ## ## $Diet ## diff lwr upr p adj ## 2-1 19.971212 -0.2998092 40.24223 0.0552271 ## 3-1 40.304545 20.0335241 60.57557 0.0000025 ## 4-1 32.617257 12.2353820 52.99913 0.0002501 ## 3-2 20.333333 -2.7268370 43.39350 0.1058474 ## 4-2 12.646045 -10.5116315 35.80372 0.4954239 ## 4-3 -7.687288 -30.8449649 15.47039 0.8277810 However, you should be screaming at me now: I did not check the assumptions! Homogeneity of variances: plot(res.aov, 1) Figure 3.18: CAPTION THIS FIGURE!! leveneTest(weight~ Diet, data = ChickWeight) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 3 9.6001 3.418e-06 *** ## 574 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We have just found what we already know. The variance is not homogeneous. The variance across groups is statistically significantly different. Normality of the residuals: plot(res.aov, 2) Figure 3.19: CAPTION THIS FIGURE!! # Extract the residuals aov_residuals &lt;- residuals(object = res.aov ) # Run Shapiro-Wilk test shapiro.test(x = aov_residuals ) ## ## Shapiro-Wilk normality test ## ## data: aov_residuals ## W = 0.94065, p-value = 2.014e-14 Non parametric then: kruskal.test(weight~ Diet, data = ChickWeight) ## ## Kruskal-Wallis rank sum test ## ## data: weight by Diet ## Kruskal-Wallis chi-squared = 24.45, df = 3, p-value = 2.012e-05 What would you conclude about this data? 3.5 Activity: Find another thing you want to test with this data. Solve this in a graphical and statistical way. Save the plots 3.6 Resources Statistics with R: https://cran.r-project.org/doc/contrib/Seefeld_StatsRBio.pdf Stat and plots with R: http://www.sthda.com/english/ "],
["sequencing-techniques-and-preprocessing.html", "Lab 4 Sequencing techniques and preprocessing 4.1 Objectives 4.2 Introduction 4.3 Different type of data (DNA, RNA, splicing, single-end paired-end) 4.4 From the sequencer machine to the sequence 4.5 General processing 4.6 Using the terminal 4.7 A bit of shell commands 4.8 Resources 4.9 Bibliography", " Lab 4 Sequencing techniques and preprocessing 4.1 Objectives After this section you should be able to: Have an overview of different sequencing techniques and different genomic data Understand the pre-processing of the data before loading it into R 4.2 Introduction Large scale sequencing techniques become more and more common as high throughput sequencing technologies became cheaper and widely available. The main objective of these techniques is to quantify the levels of different molecules: DNA, RNA and proteins. The “central dogma of molecular biology” states the main flow from the information stated on genes in the DNA to the expression of proteins. DNA is transcribed to RNA that then is translated to proteins. DNA –&gt; RNA –&gt; Protein However, this straight line misses a lot of other molecules as miRNAs, lincRNAs, transposons, etc. Moreover, not only the levels but also the quality and property of each molecule is important. One example is different RNA splicing isoforms or protein modifications. 4.3 Different type of data (DNA, RNA, splicing, single-end paired-end) It is important to know which type of sequencing techniques are available, its pros and cons and which type of molecules we are able to capture without sequencing. Even more important we need to understand the characteristics of the data we are analyzing. For example, DNA and RNA libraries differ in the fact that one undergoes splicing. That will affect the way the data is processed. Nowadays, there are two main types of libraries for nucleotides: short reads libraries and long reads libraries. Long-read techniques are more recently developed with two main technologies available: Pacific Biosciences’ (PacBio) single-molecule real-time (SMRT) sequencing and Oxford Nanopore Technologies’ (ONT) nanopore sequencing. In this book we will focus on short-read libraries, as the ones processed by Illumina machines. This technology is based on PCR amplification of the material and the base pairing with different fluorescently tagged nucleotides. Figure 1.1: Illumina sequencer strategy, adapted from Illumina user manual www.illumina.com/technology/next-generation-sequencing.html There are many library preparation protocols for Illumina sequencers. All of these techniques must include a step of fragmentation and adapter ligation. These adapters are essential to the inclusion of the fragments in the sequencer machine. Moreover, different samples might be pulled together by the use of different adapters with specific sample barcode. The idea of a barcode is the presence of specific sequences that can identify the origin of each sequence fragment. The process of separating each sample is called demultiplexing. Figure 4.1: Demultiplexing strategy Depending on the molecule of interest the library preparation to detect them will be different. For DNA the addition of the Illumina adaptors can be done by ligation or by tagmentation (which does the fragmentation and tagging at the same time). In this case, as DNA has NOT an orientation (is double stranded and palindromic) we do not care which adapter get in which side of the fragment. Figure 4.2: DNA library prep Contrary to DNA, RNA is single stranded and have a particular direction (5´-3´). It can be produced from the sense or antisense strand of the DNA which is a key property as some parts of the DNA encode different genes in each strand. Therefore, it is important to maintain the direction information from the RNA molecules. This is usually achieved thru the different 3´and 5´ Illumina adaptors. There are many types of RNA molecules with different biochemical properties. To capture each of them, there are different biochemical approaches. For small RNAs (20-30nts), usually the library starts with specific small RNA extraction methods. To capture polyadenylated mRNAs, we can add a poly A selection step using polyT oligo beads for example. If this step is done after the RNA is fragmented then this library will be 3´UTR selected. This type of libraries do not provide information about the splicing isoforms for example but it is useful to asses gene expression levels and is cheaper than multiple qPCRs. Most commonly used single cell sequencing technology use this type of libraries. To study also non-polyadenylated transcripts, as lncRNA and circRNAs, we have to deplete the libraries from ribosomal RNA which is the majority of the RNA. This can be achieved by using DNA complementary probes to rRNA followed by RNAseH digestion (cut RNA-DNA hybrids). Figure 4.3: Different RNA librarie strategies Regarding sequencing itself we can choose the length of the read and if we want to read only from side of the fragment (single-ended) of from both sides (paired-ended). Both options have different advantages. To capture different splicing patterns for example, paired-end is recommended. 4.4 From the sequencer machine to the sequence Illumina sequencing machines can be thought as fluorescence microscopes. The information they generate are basically photographs in which each color represent one nucleotide. The first step to produce the sequences is to process these images an convert them in sequences. This is done using the Illumina program bcl2fastq. This process is named basecalling and process the raw data form Illumina: basecalls files (BCL) and produces Fastq files. Figure 4.4: From BCL to Fastq Each fragment sequenced by the machine is named READ and all the reads together form a LIBRARY. Fastq format basically encodes the “name” of each read, the nucleotides and quality in ASQII format. Figure 4.5: Fastq Format The overall quality of the library can be assed using programs as FastQC. These programs read the quality and generate a overall measurement of them. Each time the machine sequences a read it evaluates the confidence it has to assign it to a particular nucleotide. This is a general idea of what the quality of each base means. Low quality reads can be filtered and low-quality bases can be trimmed from reads using tools as trimgalore or cutadapt. 4.5 General processing Once we have the sequencing data in fastq format we have to find where each read comes from. This is called alignment. The next step is to quantify how many reads comes from each place in the genome. Figure 4.6: General processing ###Alignment (bam files, annotation files, BOWTIE2 and STAR) Once we have the fastq file we have to “find” which part of the genome it is coming from. This process is called alignment. Unless we are working with uncommon organisms, most genomes are already sequenced, which means we already know the sequence of each part of the genome. Therefore, this step is almost trivial. We have to align the reads to the genome. The reads are in fastq format, and the genome are in fasta format. The fasta format start with &gt; The name of the chromosome and then it stores the sequence of the chromosome. &gt;chr1 AAATTCGGGCCAA... Maybe you have already heard about BLAST alignment. Unfortunately, this is not fast enough to aligns the millions of reads we generate from each sequencing experiment. Therefore, new techniques were developed. There are many different really good tools available. To choose one we need to consider the type of molecule we are working with and the type of sequencing technique used. It is not the same to align DNA that RNA. The main difference is the splicing awareness and the use of the gene annotation. Another thing to consider the type of sequencing performed (single vs paired end). Figure 4.7: DNA vs RNA alignement single and paired end The gene annotation files contain the information about which gene is encoded in each part of the genome. It include exons, introns and different transcript variants. There are different format of annotation files. The mostly used ones are gtf and bed12. Both contain the information about gene name, chromosome, start, end, number of exons and where each exon start. Of course, both the genome and the annotation files are updated frequently so you should consider update them as you can. Databases commonly used are ENSEMBL and UCSC. Just to show one example, gtf of fly annotation version dm6: #gff-version 2 #source-version rtracklayer 1.38.3 #date 2018-04-23 #genome-build . BDGP6 chr3R FlyBase gene 567076 2532932 . + . gene_id &quot;FBgn0267431&quot;; gene_name &quot;Myo81F&quot;; gene_source &quot;FlyBase&quot;; gene_biotype &quot;protein_coding&quot;; chr3R FlyBase transcript 567076 2532932 . + . gene_id &quot;FBgn0267431&quot;; gene_name &quot;Myo81F&quot;; gene_source &quot;FlyBase&quot;; gene_biotype &quot;protein_coding&quot;; transcript_id &quot;FBtr0392909&quot;; transcript_name &quot;Myo81F-RB&quot;; transcript_source &quot;FlyBase&quot;; transcript_biotype &quot;protein_coding&quot;; One we have aligned the data, we will have .sam files or its binary “lighter/smaller” version .bam. These files are composed of one line per read. Each line has the read names, its sequence and the location to where it was aligned it also have encoded a lot of different quality metrics including the already discussed fastq quality. Each alignment will have different alignment quality. A perfect read with a perfect match to the reference genome would give a perfect score. The presence of gaps on the alignment would reduce the alignment quality. Reads with low quality (less than 20 for example) are usually filtered and not used in downstream analysis. Here the example for two reads of the data sets used in next chapter. SRR548157.14400783 16 chr2L 24 1 36M * 0 AGAACAGATATTTAGATTGCCTCTCATTTTCTCTCC HHDHHHHHHHHFHHHHHHHHFHEHHHHHHHHHHHFG AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:36 YT:Z:UU SRR548157.4410720 0 chr2L 247 1 36M * 0 AGTGCCAACATATTGTGCTAATGAGTGCCTCTCGTT E@=?@BDDBDHGHGHDHGIGGIBGGDEGFGB@BEBD AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:36 YT:Z:UU Reads can be also aligning to one or multiple sites in the genome. This last option is usually called: multiple mapping. For repetitive sequences in the genome, as rRNA loci and transposons, it is expected that they will map to multiples places. For other loci, the multiple mapping can be an indication of low-quality reads. For RNA sequencing analysis multiple mapping reads are problematic when quantifying gene expression so sometimes they are removed from downstream analysis. Alignment files can be sorted or unsorted. This means basically if the reads are sorted in the order they were in the fastq file or in the order they appear aligned to the genome. A sorted file basically will have the reads aligned to chr1 before the ones from chr2, and so on. This is important as some tools (as the visualization) require reads to be sorted. 4.5.1 Visualization of alignment (IGV) Alignment files can be visualized using different tools as [UCSC genome browser] (https://genome.ucsc.edu/goldenPath/help/hgTrackHubHelp.html) or [IGV](https://software.broadinstitute.org/software/igv/home. IGV have the option to load your own genomes and annotation data. To load it we will need the sorted bam files. This will allow us to visualize each read with its alignment quality, gaps, etc. Figure 4.8: Alignement files (.bam) loaded into the IGV genome browser As bam files are really big there are options to store only the total count visualization. These files format can be bigwig or TDF for example. It is importan to note the different scales (just noxt to the file name). Figure 4.9: Bigwig files (.bw) loaded into the IGV genome browser. 4.5.2 Gene counts / read (esat feature counts) /peak calling Many times, it is important to account for the number of reads aligning to each part of the genome. This is what we call quantification and for each type of data we will use different quantification tools. One example of DNA sequencing data is ChipSeq data, in which we want to know to which parts of the DNA certain protein is binding to. In that case, the number of reads represent the level of binding of that protein. One tool to see this is MACS2 For RNA, the number of reads coming from one gene basically represents the level of expression of that gene. For 5´end libraries the tool we can use is ESAT. For full transcript libraries we can use tools as featurecounts. To see splicing patterns, leafcutter is one option. 4.6 Using the terminal The terminal is just a way to execute commands in your computer. It is alternatively referred to as a computer console, root console or system console. It is a non-graphical output of the computer connected with the keyboard. With it you can do anything that you would do with the mouse like for example copy a file or moving it from one location to another. Different operating systems for personal computers as Unix (linux and Mac OS) or Windows have different functioning. Most of the genomic programs are written and executed in Unix, if you have a Windows machine you can use a terminal emulator as the ones listed here. A list of useful commands can be found in many places, here one example. The first thing I would recommend you to try is to find which is your working directory (sometimes called folder). Remember, is the place in which you are located right now and where things will be executed. You can do so by typing. pwd If you want to know what is inside your current directory, you can list the files and folders (directories) by ls There are other options to visualize this. What happens if you type this? ls -l Usually commands have options and this option is usually indicated by the - sign. Lets try now something more: ls -la You can see that now it appears to be extra directories inside. This is because the option -a basically is exposing the hidden files and directories. The directory .. and . are ways to point to the directory containing the working directory and the working directory itself. I know it is complicated, so let’s see an example. What happens if you do ls -l ./ ? and ls -la? Basically you will see the same elements because ./ is the current directory. ../ is the “upper” directory, meaning the directory that contains the working directory. Try it out. To change directory, cd is your command. You just have to say were you want to go. Try: cd ../ and explore it with ls. Then you can go back into the one you were before with cd ./thenameyousawbeforewithpwd Once you know where you are and how to move, you can start creating new directories or folders with mkdir. Each command has certain things you must provide. Try to run just mkdir. You will get an error message like this usage: mkdir [-pv] [-m mode] directory .... This is literally letting you know what is the expected ways of things. Basically, it needs the name of the directory you want to create. Try now mkdir try What happens if you do ls. Do you see it now? To copy a file, we use cp. It works as follows cp whatyouwant towhereyouwantwiththenameyouwant. For example, cp try trycopy To move the location of a file you use mv what where. For example: mv try ../ Use ls ../ to see if is there. To remove it you have to use rm ../try. You will get an error message as follows: rm: try/: is a directory. To avoid it you have to use the option -r. Basically r comes from recursive and is because you want to delete the folder with all the files inside so it “recursively” removes. Try now rm -r ../try. Use ls ../ to check that it is out. You can also do cd ../ and then ls ./ to see that you removed it. To edit and create text files like scripts I use vi There are many tutorials you can find online. This is good. Just by saying vi newtext.txt, you will be in your brand-new text to edit it. Understanding the directories is important. There are a set of “special” directories I would like to mention so you just have them in mind. / – The Root Directory Is where all the things are. All you programs, folders, etc. Lets see some examples of the sub-directories you will find there. /bin – Essential User Binaries The /bin director contains the essential user binaries (programs) that must be present when the system is on. This means that for example, your operative system bash shell will be there but your google chrome no. Your installed programs will be in /usr/bin /boot – Static Boot Files The /boot directory contains the files needed to boot the system. /etc – Configuration Files /etc/ directory contains system-wide configuration files – user-specific configuration files are located in each user’s home directory /home The /home directory contains a home folder for each user. /lib – Essential Shared Libraries The /lib directory contains libraries needed by the essential binaries in the /bin and /sbin folder. Libraries needed by the binaries in the /usr/bin folder are located in /usr/lib. /tmp – Temporary Files Applications store temporary files in the /tmp directory. /usr – User Binaries &amp; Read-Only Data The /usr directory contains applications and files used by users, as opposed to applications and files used by the system. 4.7 A bit of shell commands Shell is a another language as R. As scripts in R, they also have a particular way of working. I am not going to go into detail with them but will try to give some ideas of for loops that can be useful to write scripts. Shell commands ends with .sh and to rum them you just type: sh yourshellscript.sh In a shell script you can do everything that you would do directly in the terminal. You can create them by vi myshellscript.sh and then adding things like: mkdir AAA Remember, to edit the text you will have to use the insert mode (presing the key A for example). To save your changes you have to be out of the insert mode (pressing scape key) and then :w. This will “write” the changes. Then you can exit the editor with :q. If you want to quit without saving you can press :q!. Try it out and see what happened using ls for example. Then you can execute the script by typing sh myshellscript.sh What do you expect to happened now? Do you see any changes with ls? The asterisk * is used to complete “any chararcters” ls tr* Will list any file that start with tr. You could then list files ending with “.bam” to list the alignment files: ls *.bam For loops in shell works as follows: for list_files do something you want to do done Variables in shell are created just with the = sign VARIABLE=somevalue And can be called using $VARIABLE. Try this in your terminal X=2 $X We can now integrate all this and see one example. This is an example of a loop to create the tdf file from each bam file: for FILE in *.bam #This will go over all the files that ends with .bam and will store them in a variable named FILE do NAME=`echo $FILE | cut -d &#39;bam&#39; -f1` #it creates a name using the function cut, just remove the bam echo &quot;##############Running sample: $NAME##############&quot; #echo is to print things in the terminal igvtools count -w 5 --minMapQuality 20 --strands read &quot;$FILE&quot; ./&quot;$FILE&quot;.tdf genome.fa&quot; done 4.8 Resources 4.9 Bibliography Griffith M, Walker JR, Spies NC, Ainscough BJ, Griffith OL (2015) Informatics for RNA Sequencing: A Web Resource for Analysis on the Cloud. PLoS Comput Biol 11(8): e1004393. https://doi.org/10.1371/journal.pcbi.1004393 An introduction to Next-Generation Sequencing Technology, Illumina, available at www.illumina.com/technology/next-generation-sequencing.html Alberts B, Johnson A, Lewis J, et al. Molecular Biology of the Cell. 4th edition. New York: Garland Science; 2002. From DNA to RNA. Available from: https://www.ncbi.nlm.nih.gov/books/NBK26887/ Amarasinghe, S.L., Su, S., Dong, X. et al. Opportunities and challenges in long-read sequencing data analysis. Genome Biol 21, 30 (2020). https://doi.org/10.1186/s13059-020-1935-5 https://www.howtogeek.com/117435/htg-explains-the-linux-directory-structure-explained/ "]
]
